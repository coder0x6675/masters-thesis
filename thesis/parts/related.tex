\chapter{Related Work}
\label{sec:relatedwork}

\section{Deep learning for lightning strike prediction}

\textcite{bib5} explores the prediction of lightning strikes using deep learning (DL) models. To gather the necessary data, the author sets up custom weather stations that collect common weather parameters such as air pressure, temperature, and wind speed.

The study employs two types of DL models: Long Short-Term Memory (LSTM) and Temporal Convolutional Neural (TCN) models. LSTM models have an advantage over simple recurrent neural networks (RNNs) as they overcome the vanishing gradient problem. This problem arises when the importance of past time steps diminishes over time, resulting in older time points having minimal impact on predictions \cite{rnn-vanishing-gradient}. LSTM layers address this issue by incorporating forget, input, and output gates, which determine the relevance of information for future predictions. The TCN model is an improvement over the LSTM model, combining convolutional and recurrent capabilities to enable simpler autoregressive modeling and the utilization of longer memory sequences.

Before training the models, the author applies several pre-processing techniques to prepare the original observations. The data is collected over an 8-month period at 15-minute intervals. To handle missing values, linear interpolation is used for data imputation. Additionally, all parameters are scaled to a range between 0 and 1. To extract training labels, a sliding-window procedure is employed, using the last 7 days of data as the feature sequence and predicting the label two hours ahead.

To evaluate the model's performance, the author compares the Multiple-Input Multiple-Output (MIMO) and Multiple-Input Single-Output (MISO) metrics. The resulting model demonstrates accurate predictions for 10 different parameters while maintaining computational efficiency, enabling it to be run on stand-alone PCs.

%While the paper provides valuable suggestions of input parameters, model types and evaluation methods, it has shortcommings that needs to be addressed. The dataset was compiled using limited hardware and resources, under a relatively short timeframe. This study aims to solve these shortcommings by utilizing a high quality dataset collected by a recognized governmental agency with access to proper resources and expertise.

In a separate study \textcite{new1} explores the use of deep learning techniques, specifically Feed-Forward Neural Networks (FFNN), to predict lightning occurrences within a 100 km radius from University Malaysia Pahang Al-Sultan Abdullah (UMPSA) in Pekan, Pahang, Malaysia. The study leverages data recorded by the Malaysia Meteorology Department (MET Malaysia) and aims to develop a model that can predict the latitude and longitude of lightning occurrences using historical data.

The article provides a comprehensive review of previous work in lightning prediction and compares different machine learning and deep learning approaches. The methodology section explains the use of FFNN, activation functions, optimization techniques, and data preparation. The results and discussion section evaluates the performance of the FFNN model using various metrics and compares it with an LSTM model. The article also includes detailed simulation results and visualizations to demonstrate the effectiveness of the FFNN model.

The study concludes that the FFNN model is effective in predicting lightning occurrences and suggests future work to improve the model's accuracy. The article's key contributions include a novel application of FFNN for lightning prediction, a comprehensive evaluation of the model's performance, and practical implications for improving lightning prediction accuracy.

In a similar study \textcite{bib10} made an attempt to use machine learning techniques for short-term prediction of lightning strikes. They collected a comprehensive dataset of 20,000,000 lightning strikes, covering the entire country of South Africa. The dataset was pre-processed using a binning technique with a temporal bin size of 3 hours, and the parameters were scaled to fall in between 0 and 1. The dataset was divided into a training set and a testing set, with a ratio of 70\% for the training and 30\% for the testing data. Three different machine learning models were evaluated and compared: an AutoRegressive (AR) model, an AutoRegressive Integrated Moving Average (ARIMA) model, and a Long Short Term Memory (LSTM) model. Among these models, only the LSTM model is a deep learning model. The results demonstrated that the LSTM model considerably outperformed AR and ARIMA in terms of Mean Absolute Percentage Error (MAPE), although all models showed high error rates.

%Notably, the study tested the models based on purely statistical and empirical data without considering climate or weather-related parameters. This may be a possible reason for the high error rates observed. Additionally they used a single DL model, not enough for a comparison. Nevertheless, the study demonstrated that deep learning models may be more efficient than other types of machine learning models, and it provides several suggestions for useful data pre-processing methods, a baseline train/test ratio, and model hyperparameters.

\section{Model Types}

\textcite{bib4} and \textcite{new5} provide a comprehensive overview of the current state of deep learning in weather prediction and compare the results with traditional numerical methods. One particularly successful type of DL model is the convolutional neural network (CNN) model \cite{s40}. CNNs are designed to process two-dimensional data, such as satellite imagery or weather model output, by applying a series of filters to extract increasingly complex patterns. By learning higher-level spatial behavior from the input data, CNNs can effectively predict weather phenomena.

Another type of DL model that has been explored is the recurrent neural network (RNN) \cite{s43} \cite{s44} \cite{s45}. RNNs are essentially dense neural networks with additional functionality built into the layers, allowing them to analyze input data in temporal sequences. Unlike CNNs, RNNs can take the dimension of time into account. They not only extract patterns based on the provided feature sequence but also learn from the changes in the data over time. This might make RNNs particularly suitable for capturing the dynamics of weather patterns.

In addition to CNNs and RNNs, more advanced model types such as variational auto-encoders (VAEs) and generative adversarial networks (GANs) have also been attempted \cite{s47} \cite{s48}. VAEs encode and decode the input data in specific ways to extract the important features for the task at hand. GANs, on the other hand, use two competitive neural networks: one that generates predictions and another that learns to differentiate between artificially generated predictions and true predictions. By leveraging the interplay between these two networks, GANs can improve the accuracy of the predictions.

\textcite{new2} also explores the use of LSTM recurrent neural networks to predict short-term lightning flash densities in Southern Africa. The study focuses on two specific areas with different lightning flash densities. The authors evaluated the prediction ability of the LSTM model using historical data from the South African Lightning Detection Network. The model was trained using four years of data and predictions were made for one-year intervals. The results showed good correlation between predicted and actual lightning flashes, but the model tended to under-predict in one area. The model also showed good repeatability and consistent mean absolute error values. However, it did not predict about 60\% of major lightning events. The study suggests combining the LSTM model with a weather data model to improve accuracy.

\textcite{new3} presents a deep learning-based lightning prediction system that utilizes atmospheric electric field (EF) observations to predict the occurrence and location of lightning events. The study collected data from EF measurement stations and lightning locators in Guangzhou city. The data were clustered into one-minute intervals, and thunderstorm and non-thunderstorm samples were created for analysis.

To extract features from the EF time series data, a Sparse Autoencoder (SAE) was employed, which compresses the dimensionality of the input data while retaining essential information. The time positioning module utilized an improved ResNet50 model to classify weather samples and predict the occurrence of lightning. The model achieved an accuracy of 88.2\%, with a precision of 92.2\%, recall of 81.5\%, and F1-score of 86.4\%. The length of the EF time series was found to impact the model's accuracy, with a 60-minute time series yielding the best results.

Data augmentation techniques were applied to increase the training data and improve the model's performance. The improved ResNet50 model outperformed other models, such as CNN and LSTM, in terms of precision, recall, F1-score, and accuracy. The model also demonstrated robustness against noise interference, maintaining high accuracy even under noisy conditions.

For spatial positioning, an MLP model was used to predict the location of lightning occurrences based on EF data. The model achieved satisfactory results, accurately predicting the location of lightning flashes with most errors within acceptable limits.

The study evaluated the effect of different lengths of EF time series on the accuracy of lightning spatial localization and found that the length had little impact on accuracy. Additionally, real-world case studies were presented to demonstrate the model's performance in practical scenarios, showcasing its ability to accurately predict lightning occurrences and provide valuable information for lightning protection measures.

\section{Feature Selection}

\textcite{new4} introduces a novel approach for feature selection in classification tasks using neural networks. The method involves training neural networks with an augmented cross-entropy error function to improve generalization and robustness. The authors propose a feature selection procedure that ranks features based on their impact on classification error and selects the subset with the highest accuracy. The proposed method is compared to five other techniques and consistently outperforms them in terms of classification accuracy. The five other feature selection techniques were Neural-Network Feature Selector (NNFS), Signal-to-Noise Ratio (SNR) Based Technique, Neural Network Output Sensitivity Based Feature Ranking, Fuzzy Entropy Based Feature Ranking, and Discriminant Analysis (DA) Based Feature Ranking. 

Experimental investigations on artificial and real-world datasets demonstrate the effectiveness and robustness of the approach. The selected feature subsets also yield the best performance when tested with the k-Nearest Neighbors classifier.

\textcite{pca-performance} provides a potential method for finding the optimal features. The authors evaluate the use of Principal Component Analysis (PCA) to improve the performance of machine learning methods in classifying data of higher dimensions, such as Raman spectra used for identifying narcotics. The presence of redundant or highly correlated attributes in such data can degrade classification accuracy and model performance. 

The experiments show that PCA can improve the performance of machine learning in classifying high dimensional data. The study also compares the performance of five well-known machine learning techniques (Support Vector Machines, k-Nearest Neighbors, C4.5 Decision Tree, RIPPER and Naive Bayes) along with classification by Linear Regression on a Raman spectral dataset. 

The results show that Support Vector Machines perform better than other methods on raw and normalized data, while pre-processing techniques such as normalization and first derivative can improve the classification accuracy of these methods. The use of PCA in combination with machine learning methods also improves performance, with most methods requiring no more than six principal components to achieve the lowest error. This method could aid in the identification and selection of the optimal parameters for lightning prediction.

However, the study also finds that the performance of rule-based learners, such as C4.5 and RIPPER, can be adversely affected by the use of PCA. The paper concludes that the use of NIPALS PCA in combination with machine learning appears to be a promising approach for the classification of high dimensional spectral data.

\section{Hyperparameter Tuning}

Choosing the appropriate hyperparameters for a model is a common problem in machine learning. A model typically consists of two types of parameters: internal and external hyperparameters. The internal parameters are automatically optimized during the training process. On the other hand, the external hyperparameters are part of the model and cannot be changed once the model has been built.

To find the optimal hyperparameters, different strategies and methods have been proposed in literature. 
\textcite{genetic-algorithms} presents an extensive overview and comparison of the most common strategies and methods for finding the optimal hyperparameters are provided. The described strategies include random search, grid search, Bayesian search and genetic algorithms, however only the latter three are evaluated and compared. A brief overview of the strategies follows:

\begin{description}
	\item[Random Search:] A simple strategy for hyperparameter tuning where hyperparameters are randomly chosen from a predefined set. The model is trained and evaluated using the selected hyperparameters, and this process is repeated for a specific number of iterations or until a sufficient performance is achieved. Random search is computationally efficient but may not always find the optimal set of hyperparameters due to its random nature.
	\item[Grid Search:] A systematic approach to hyperparameter tuning where a predefined set of hyperparameters is specified, and the model is trained and evaluated for each combination of hyperparameters in the grid. Grid search fully explores all possible hyperparameter combinations, making it guaranteed to find the optimal combination. However, it can be extremely computationally expensive, especially with a large number of hyperparameters.
	\item[Bayesian Optimization:] A sequential optimization strategy that uses a probabilistic model to guide the search for optimal hyperparameters. It combines prior knowledge and observed performance to update a probability distribution over the hyperparameters. Bayesian optimization can handle noisy and expensive-to-evaluate objective functions and adapts its search based on observed performance. However, it may require more iterations to converge compared to other strategies.
	\item[Genetic Algorithms:] Genetic algorithms generate a population of potential candidates (combinations of hyperparameters). These candidates are evaluated, and the best-performing ones produce offspring for the next generation through crossover and mutation. This process is repeated for a specified number of generations or until sufficient performance is achieved. Genetic algorithms can explore a large search space and potentially find global optima but can be computationally expensive.
\end{description}

The evaluated hyperparameters were layer number and size, optimizer, loss function, activation function, dropout and validation dataset split. The results were similar, however the Bayesian algorithm performed better than the grid search, with the genetic algorithm showing the highest efficiency.

Additionally, \textcite{new6} provides a comprehensive review of Hyper-Parameter Optimization (HPO) in deep neural networks. It discusses key hyper-parameters, such as learning rate and optimizer, and explores various search algorithms and trial schedulers for HPO, including grid search, random search and Bayesian optimization. The paper also compares different toolkits and services for HPO, including Google Vizier, Amazon SageMaker, NNI, and Ray.Tune. It concludes by discussing the comparability of different algorithms and evaluation methods for HPO. The paper emphasizes the importance of computational efficiency and effective evaluation in HPO for deep learning networks.

\section{Model Evaluation and Metrics}

\textcite{evaluation-metrics} provide a detailed description of accuracy measures and evaluate the performance of the prediction models using a dataset from the UCI Machine Learning Repository. They argue that there is no universally accepted single metric for choosing the appropriate forecasting method, and that different approaches can perform differently depending on the chosen metric. Therefore, they propose a framework that considers various accuracy metrics concurrently, providing a more robust comparability of classification methodologies.

The authors conclude by suggesting that the proposed multidimensional framework provides more robust rankings than one-dimensional analysis, and can be further improved by considering other aspects of classification algorithms such as ease of use, computational costs, and flexibility. In the context of this study, it is used to highlight the importance of including multiple metrics for accuracy and efficiency, and provides an overview of which ones to choose.

\section{Research Gap}

The art of lightning prediction using deep learning models has made significant progress, yet several critical research gaps remain that would require further exploration.

Firstly, there is a lack of comprehensive model comparison and benchmarking. While various architectures, such as LSTM, TCN, and FFNN have been utilized, most studies focus on the performance of a single model. A systematic comparison of multiple models under consistent conditions would provide valuable insights into their practical usability.

Secondly, the challenge of imbalanced datasets is significant, as lightning strike events are rare compared to non-strike events. Current literature often overlooks strategies to address this imbalance, such as oversampling, undersampling, or specialized loss functions, which could enhance model performance and reliability.

Additionally, there is a need for more in-depth analysis of feature selection and the optimal hyperparameters. While some studies touch upon these topics, they often lack a thorough examination of which meteorological parameters or hyperparameters are most predictive of lightning strikes. Understanding this relevance could lead to more effective models.

